from flask import Flask, request, jsonify, render_template
import pandas as pd
import numpy as np
import joblib
import os
import json
import logging

# Import model classes
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
import lightgbm as lgbm # For lgbm.Booster and LGBMClassifier (though we primarily use Booster for prediction here)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# --- Basic Logging Configuration --- ### <<< MODIFIED LOGGING
# Ensure logging is configured right at the start
# Set a basic format; Gunicorn will often override parts of this but it helps for local.
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(name)s %(module)s %(funcName)s: %(message)s')
logger = logging.getLogger(__name__) # ### <<< ADDED LOGGING (Get a specific logger)
logger.info("--- Logger initialized ---") # ### <<< ADDED LOGGING

app = Flask(__name__)
logger.info("--- Flask app instance created ---") # ### <<< ADDED LOGGING

# Configure base directory for models
# Assumes 'model_output' (generated by training script) is in the same directory as app.py
# Or, provide an absolute path.
MODEL_BASE_DIR = os.path.join(os.path.dirname(__file__), 'model_output')
TARGET_COLUMN = 'hp_category' # Used for metrics calculation if present in uploaded CSV

logger.info(f"--- SCRIPT START: __file__ is {__file__} ---") # ### <<< ADDED LOGGING
logger.info(f"--- SCRIPT START: os.path.dirname(__file__) is {os.path.dirname(__file__)} ---") # ### <<< ADDED LOGGING
logger.info(f"--- MODEL_BASE_DIR configured to: {MODEL_BASE_DIR} ---") # ### <<< ADDED LOGGING
logger.info(f"--- Absolute path for MODEL_BASE_DIR: {os.path.abspath(MODEL_BASE_DIR)} ---") # ### <<< ADDED LOGGING
if not os.path.isdir(MODEL_BASE_DIR): # ### <<< ADDED LOGGING
    logger.error(f"--- CRITICAL: MODEL_BASE_DIR does not exist or is not a directory: {os.path.abspath(MODEL_BASE_DIR)} ---") # ### <<< ADDED LOGGING
else: # ### <<< ADDED LOGGING
    logger.info(f"--- SUCCESS: MODEL_BASE_DIR exists: {os.path.abspath(MODEL_BASE_DIR)} ---") # ### <<< ADDED LOGGING
    logger.info(f"--- Contents of MODEL_BASE_DIR: {os.listdir(MODEL_BASE_DIR)} ---") # ### ADDED LOGGING


# --- Model Configuration ---
# Maps frontend model ID to file names and loading specifics
# Note: model_subdir should match the subdirectories created by your training script
MODEL_CONFIG = {
    "catboost": {
        "model_subdir": "catboost_model", # Subdirectory name under MODEL_BASE_DIR
        "model_file": "catboost_model.cbm",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": CatBoostClassifier,
        "load_method": "load_model", # native CatBoost load
        "is_sklearn_joblib": False
    },
    "xgboost": {
        "model_subdir": "xgboost_model",
        "model_file": "xgboost_model.json", # XGBoost can save/load as JSON
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": XGBClassifier,
        "load_method": "load_model", # native XGBoost load
        "is_sklearn_joblib": False
    },
    "random_forest": {
        "model_subdir": "randomforest_model",
        "model_file": "randomforest_model.joblib",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": RandomForestClassifier, # Not actually used for joblib, but good for reference
        "load_method": "joblib",
        "is_sklearn_joblib": True
    },
    "lightgbm": {
        "model_subdir": "lightgbm_model",
        "model_file": "lightgbm_model.txt", # Saved as text by booster_.save_model()
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": None, # We load Booster directly, so specific class not used for instantiation here
        "load_method": "lightgbm_booster",
        "is_sklearn_joblib": False
    },
    "gradient_boosting": {
        "model_subdir": "gradientboosting_model",
        "model_file": "gradientboosting_model.joblib",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": GradientBoostingClassifier, # Not used for joblib
        "load_method": "joblib",
        "is_sklearn_joblib": True
    }
}
logger.info("--- MODEL_CONFIG defined ---") # ### <<< ADDED LOGGING

# Cache for loaded models to avoid reloading on every request
loaded_models_cache = {}
logger.info("--- loaded_models_cache initialized ---") # ### <<< ADDED LOGGING

def load_model_components(model_id):
    logger.debug(f"load_model_components: called for model_id: {model_id}") # ### <<< ADDED LOGGING
    """Loads model, scaler, and feature names for a given model_id."""
    if model_id in loaded_models_cache:
        logger.info(f"Using cached components for model_id: {model_id}")
        return loaded_models_cache[model_id]

    if model_id not in MODEL_CONFIG:
        logger.error(f"load_model_components: Invalid model_id: {model_id}. Not found in MODEL_CONFIG.") # ### <<< ADDED LOGGING
        raise ValueError(f"Invalid model_id: {model_id}. Not found in MODEL_CONFIG.")

    config = MODEL_CONFIG[model_id]
    model_subdir_path = os.path.join(MODEL_BASE_DIR, config["model_subdir"])
    logger.debug(f"load_model_components: model_subdir_path: {model_subdir_path} (abs: {os.path.abspath(model_subdir_path)})") # ### <<< ADDED LOGGING

    model_path = os.path.join(model_subdir_path, config["model_file"])
    scaler_path = os.path.join(model_subdir_path, config["scaler_file"])
    features_path = os.path.join(model_subdir_path, config["features_file"])
    logger.debug(f"load_model_components: model_path: {model_path} (abs: {os.path.abspath(model_path)})") # ### <<< ADDED LOGGING
    logger.debug(f"load_model_components: scaler_path: {scaler_path} (abs: {os.path.abspath(scaler_path)})") # ### <<< ADDED LOGGING
    logger.debug(f"load_model_components: features_path: {features_path} (abs: {os.path.abspath(features_path)})") # ### <<< ADDED LOGGING


    if not os.path.exists(model_path):
        logger.error(f"load_model_components: Model file not found: {model_path} (abs: {os.path.abspath(model_path)})") # ### <<< ADDED LOGGING
        raise FileNotFoundError(f"Model file not found: {model_path}")
    if not os.path.exists(scaler_path):
        logger.error(f"load_model_components: Scaler file not found: {scaler_path} (abs: {os.path.abspath(scaler_path)})") # ### <<< ADDED LOGGING
        raise FileNotFoundError(f"Scaler file not found: {scaler_path}")
    if not os.path.exists(features_path):
        logger.error(f"load_model_components: Features file not found: {features_path} (abs: {os.path.abspath(features_path)})") # ### <<< ADDED LOGGING
        raise FileNotFoundError(f"Features file not found: {features_path}")
    
    logger.info(f"load_model_components: All component files exist for model_id: {model_id}. Proceeding to load.") # ### <<< ADDED LOGGING

    model_instance = None
    try: # ### <<< ADDED LOGGING (try-except around model loading)
        if config["load_method"] == "load_model": # CatBoost, XGBoost
            logger.debug(f"load_model_components: Loading {model_id} using 'load_model' method.") # ### <<< ADDED LOGGING
            ModelClass = config["loader_class"]
            model_instance = ModelClass()
            model_instance.load_model(model_path)
        elif config["load_method"] == "joblib": # RandomForest, GradientBoosting
            logger.debug(f"load_model_components: Loading {model_id} using 'joblib' method.") # ### <<< ADDED LOGGING
            model_instance = joblib.load(model_path)
        elif config["load_method"] == "lightgbm_booster":
            logger.debug(f"load_model_components: Loading {model_id} using 'lightgbm_booster' method.") # ### <<< ADDED LOGGING
            model_instance = lgbm.Booster(model_file=model_path)
        else:
            logger.error(f"load_model_components: Unknown load_method: {config['load_method']} for model_id: {model_id}") # ### <<< ADDED LOGGING
            raise ValueError(f"Unknown load_method: {config['load_method']} for model_id: {model_id}")
        logger.info(f"load_model_components: Model instance for {model_id} loaded successfully.") # ### <<< ADDED LOGGING
    except Exception as e_model_load: # ### <<< ADDED LOGGING
        logger.error(f"load_model_components: FAILED to load model instance for {model_id} from {model_path}. Error: {e_model_load}", exc_info=True) # ### <<< ADDED LOGGING
        raise # ### <<< ADDED LOGGING (re-raise the exception)

    try: # ### <<< ADDED LOGGING (try-except around scaler loading)
        logger.debug(f"load_model_components: Loading scaler for {model_id} from {scaler_path}.") # ### <<< ADDED LOGGING
        scaler_instance = joblib.load(scaler_path)
        logger.info(f"load_model_components: Scaler for {model_id} loaded successfully.") # ### <<< ADDED LOGGING
    except Exception as e_scaler_load: # ### <<< ADDED LOGGING
        logger.error(f"load_model_components: FAILED to load scaler for {model_id} from {scaler_path}. Error: {e_scaler_load}", exc_info=True) # ### <<< ADDED LOGGING
        raise # ### <<< ADDED LOGGING

    try: # ### <<< ADDED LOGGING (try-except around features loading)
        logger.debug(f"load_model_components: Loading features for {model_id} from {features_path}.") # ### <<< ADDED LOGGING
        with open(features_path, 'r') as f:
            feature_names_list = json.load(f)
        logger.info(f"load_model_components: Features for {model_id} loaded successfully.") # ### <<< ADDED LOGGING
    except Exception as e_features_load: # ### <<< ADDED LOGGING
        logger.error(f"load_model_components: FAILED to load features for {model_id} from {features_path}. Error: {e_features_load}", exc_info=True) # ### <<< ADDED LOGGING
        raise # ### <<< ADDED LOGGING


    logger.info(f"{model_id.capitalize()} model, scaler, and features loaded and processed successfully.") # ### <<< MODIFIED LOGGING
    
    components = (model_instance, scaler_instance, feature_names_list)
    loaded_models_cache[model_id] = components # Cache it
    logger.debug(f"load_model_components: Components for {model_id} cached.") # ### <<< ADDED LOGGING
    return components

@app.route('/')
def index():
    logger.info("--- Route / called ---") # ### <<< ADDED LOGGING
    """Serves the main HTML page."""
    template_path_check = os.path.join(app.root_path, 'templates', 'index.html')
    logger.debug(f"Attempting to render template: index.html")
    logger.debug(f"app.root_path is: {app.root_path} (abs: {os.path.abspath(app.root_path)})") # ### <<< ADDED LOGGING
    logger.debug(f"Full path to template being considered: {template_path_check} (abs: {os.path.abspath(template_path_check)})") # ### <<< MODIFIED LOGGING
    
    if not os.path.exists(template_path_check):
        logger.error(f"Template file DOES NOT EXIST at: {template_path_check} (abs: {os.path.abspath(template_path_check)})") # ### <<< MODIFIED LOGGING
        return "Error: Template file 'index.html' not found in 'templates' folder.", 500
    try:
        logger.info("Rendering template index.html") # ### <<< ADDED LOGGING
        return render_template('index.html')
    except Exception as e_render:
        logger.error(f"Error during render_template('index.html'): {e_render}", exc_info=True)
        return f"Error rendering template: {e_render}", 500

@app.route('/predict', methods=['POST'])
def predict():
    logger.info("--- Route /predict called ---") # ### <<< ADDED LOGGING
    """Handles file upload, prediction, and metrics calculation."""
    if 'file' not in request.files:
        logger.warning("/predict: 'file' not in request.files") # ### <<< ADDED LOGGING
        return jsonify({"error": "No file part in the request"}), 400
    file = request.files['file']
    if file.filename == '':
        logger.warning("/predict: No file selected for uploading (file.filename is empty)") # ### <<< ADDED LOGGING
        return jsonify({"error": "No file selected for uploading"}), 400

    model_id_from_request = request.form.get('model_id', 'catboost') # Default to catboost if not provided
    logger.info(f"Received prediction request for model_id: {model_id_from_request}")

    try:
        logger.debug(f"/predict: Attempting to load model components for {model_id_from_request}") # ### <<< ADDED LOGGING
        model, scaler, feature_names = load_model_components(model_id_from_request)
        logger.info(f"/predict: Model components for {model_id_from_request} loaded successfully.") # ### <<< ADDED LOGGING
    except FileNotFoundError as e_fnf: # ### <<< ADDED LOGGING (specific catch for FileNotFoundError)
        logger.error(f"/predict: FileNotFoundError while loading components for model {model_id_from_request}: {e_fnf}", exc_info=True) # ### <<< ADDED LOGGING
        return jsonify({"error": f"Model files missing for '{model_id_from_request}'. Server configuration issue or files not deployed. Details: {str(e_fnf)}"}), 500 # ### <<< ADDED LOGGING
    except Exception as e_load:
        logger.error(f"/predict: Error loading components for model {model_id_from_request}: {e_load}", exc_info=True)
        return jsonify({"error": f"Could not load model '{model_id_from_request}'. Server not ready or model files missing. Details: {str(e_load)}"}), 500

    if not model or not scaler or not feature_names: # Should be caught by exception above, but as a safeguard
        logger.error(f"/predict: Model components for '{model_id_from_request}' are None after load_model_components call. This should not happen.") # ### <<< ADDED LOGGING
        return jsonify({"error": f"Model components for '{model_id_from_request}' are not properly loaded."}), 500


    if file and file.filename.endswith('.csv'):
        try:
            logger.debug(f"/predict: Processing CSV file: {file.filename}") # ### <<< ADDED LOGGING
            input_df = pd.read_csv(file.stream)
            original_input_df_for_metrics = input_df.copy()
            logger.debug(f"/predict: CSV read successfully. Shape: {input_df.shape}") # ### <<< ADDED LOGGING
            
            missing_cols = [col for col in feature_names if col not in input_df.columns]
            if missing_cols:
                logger.warning(f"/predict: Missing required columns in CSV: {', '.join(missing_cols)}") # ### <<< ADDED LOGGING
                return jsonify({"error": f"Missing required columns in CSV: {', '.join(missing_cols)}"}), 400
            
            processed_df = input_df[feature_names].copy()
            for col in feature_names:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')

            if processed_df.isnull().any().any():
                nan_feature_cols = processed_df.columns[processed_df.isnull().any()].tolist()
                logger.warning(f"NaN values found in feature columns: {nan_feature_cols}. Applying fillna(0) as default imputation. Ensure this matches training.")
                processed_df.fillna(0, inplace=True) # Basic imputation

            logger.debug("/predict: Scaling features...") # ### <<< ADDED LOGGING
            scaled_features = scaler.transform(processed_df)
            logger.debug("/predict: Features scaled. Predicting...") # ### <<< ADDED LOGGING
            
            # Prediction logic adaptable to different model types
            raw_predictions_from_model = model.predict(scaled_features)
            logger.info(f"/predict: Raw predictions received from model {model_id_from_request}. Shape: {raw_predictions_from_model.shape if hasattr(raw_predictions_from_model, 'shape') else 'N/A'}") # ### <<< ADDED LOGGING
            
            predictions = None # Initialize

            if model_id_from_request == "lightgbm" and isinstance(model, lgbm.Booster):
                if raw_predictions_from_model.ndim == 2 and raw_predictions_from_model.shape[1] > 1:
                    logger.info(f"LightGBM booster returned 2D array, shape: {raw_predictions_from_model.shape}. Applying argmax for class labels.")
                    predictions = np.argmax(raw_predictions_from_model, axis=1)
                elif raw_predictions_from_model.ndim == 1:
                    logger.warning(f"LightGBM booster returned 1D array, shape: {raw_predictions_from_model.shape}. Assuming direct class labels or needs thresholding if binary probabilities.")
                    predictions = raw_predictions_from_model 
                else:
                    logger.error(f"Unexpected prediction shape from LightGBM booster: {raw_predictions_from_model.shape}")
                    predictions = raw_predictions_from_model 

            elif isinstance(model, CatBoostClassifier):
                if raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] == 1:
                    logger.info(f"CatBoost returned 2D array with 1 col, shape: {raw_predictions_from_model.shape}. Flattening.") # ### <<< ADDED LOGGING
                    predictions = raw_predictions_from_model.flatten()
                elif raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] > 1:
                    logger.info(f"CatBoost returned multi-column output for predict, shape: {raw_predictions_from_model.shape}. Applying argmax.")
                    predictions = np.argmax(raw_predictions_from_model, axis=1)
                else: # Standard 1D array
                    logger.info(f"CatBoost returned 1D array, shape: {raw_predictions_from_model.shape}.") # ### <<< ADDED LOGGING
                    predictions = raw_predictions_from_model
            
            elif hasattr(model, 'classes_') and raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] > 1 : 
                logger.warning(f"Model {model_id_from_request} ({type(model)}) returned multi-column output for .predict(), shape: {raw_predictions_from_model.shape}. Applying argmax.")
                predictions = np.argmax(raw_predictions_from_model, axis=1)
            
            else: # Standard 1D array of class labels from other models (XGBoost, RF, GB usually do this)
                logger.info(f"Model {model_id_from_request} ({type(model)}) returned standard 1D output or already processed. Shape: {raw_predictions_from_model.shape if hasattr(raw_predictions_from_model, 'shape') else 'N/A'}") # ### <<< ADDED LOGGING
                predictions = raw_predictions_from_model

            if predictions is not None:
                predictions = np.array(predictions).flatten().astype(int)
                logger.info(f"/predict: Final predictions processed. Shape: {predictions.shape}") # ### <<< ADDED LOGGING
            else:
                logger.error(f"Predictions array is None after model-specific handling for {model_id_from_request}. This indicates an issue.")
                return jsonify({"error": f"Failed to derive class labels from model {model_id_from_request} output."}), 500

            results_df_for_display = input_df.copy() 
            results_df_for_display['Predicted_Class'] = predictions.tolist()
            response_data = results_df_for_display.to_dict(orient='records')
            
            metrics = None
            if TARGET_COLUMN in original_input_df_for_metrics.columns:
                try:
                    logger.debug(f"/predict: Target column '{TARGET_COLUMN}' found. Calculating metrics.") # ### <<< ADDED LOGGING
                    # ... (rest of your metrics calculation logic) ...
                    # (Consider adding more logs within the metrics calculation if needed)
                    logger.info(f"/predict: Metrics calculation complete for {model_id_from_request}.") # ### <<< ADDED LOGGING (at the end of successful metrics calculation)
                except Exception as e_metrics:
                    logger.error(f"Error calculating metrics for {model_id_from_request}: {e_metrics}", exc_info=True)
                    metrics = {"error": f"Could not calculate metrics: {str(e_metrics)}"}
            else:
                logger.info(f"/predict: Target column '{TARGET_COLUMN}' not found in uploaded file. Metrics not calculated.") # ### <<< ADDED LOGGING
                metrics = {"message": f"Target column '{TARGET_COLUMN}' not found in uploaded file. Metrics not calculated."}

            logger.info(f"/predict: Returning prediction response for {model_id_from_request}.") # ### <<< ADDED LOGGING
            return jsonify({
                "predictions": response_data, 
                "headers": results_df_for_display.columns.tolist(),
                "metrics": metrics,
                "model_used": model_id_from_request
            })

        except pd.errors.EmptyDataError:
            logger.error("/predict: Uploaded CSV file is empty.", exc_info=True) # ### <<< ADDED LOGGING
            return jsonify({"error": "Uploaded CSV file is empty."}), 400
        except Exception as e_process:
            logger.error(f"Error during prediction processing for {model_id_from_request}: {e_process}", exc_info=True)
            return jsonify({"error": f"An error occurred: {str(e_process)}"}), 500
    else:
        logger.warning(f"/predict: Invalid file type received: {file.filename if file else 'No file'}") # ### <<< ADDED LOGGING
        return jsonify({"error": "Invalid file type. Please upload a CSV file."}), 400

if __name__ == '__main__':
    # This block is mainly for local development. Gunicorn doesn't use it directly.
    # But it's good to have robust logging here too.
    logger.info("--- Flask App Starting (direct run via __main__) ---")
    logger.info(f"--- Local Dev: Models will be loaded from: {MODEL_BASE_DIR} (abs: {os.path.abspath(MODEL_BASE_DIR)}) ---")
    if not os.path.isdir(MODEL_BASE_DIR):
         logger.critical(f"--- Local Dev CRITICAL: MODEL_BASE_DIR does not exist or is not a directory: {os.path.abspath(MODEL_BASE_DIR)} ---")
    else:
        logger.info(f"--- Local Dev SUCCESS: MODEL_BASE_DIR exists: {os.path.abspath(MODEL_BASE_DIR)} ---")
        logger.info(f"--- Local Dev Contents of MODEL_BASE_DIR: {os.listdir(MODEL_BASE_DIR)} ---")

    logger.info(f"--- Local Dev: To run, ensure the 'model_output' directory (from training) is present here or MODEL_BASE_DIR is correctly set.")
    logger.info(f"--- Local Dev: Expected subdirectories in '{MODEL_BASE_DIR}': {', '.join([MODEL_CONFIG[key]['model_subdir'] for key in MODEL_CONFIG])}")
    # For local development, you might want to use a different port than Render's default 10000
    local_port = int(os.environ.get("PORT", 5001)) # ### <<< MODIFIED LOGGING (different local port)
    logger.info(f"--- Local Dev: Starting Flask development server on host 0.0.0.0, port {local_port} ---") # ### <<< ADDED LOGGING
    app.run(debug=False, host='0.0.0.0', port=local_port) # Set debug=False to better mimic production logging
else:
    # This block executes when Gunicorn imports the app
    logger.info("--- Flask App Initializing (imported by WSGI server like Gunicorn) ---")
    # Potentially pre-load models here if desired, but be mindful of worker startup times
    # For now, models are loaded on-demand by load_model_components
    # Example: You could try to load a default model to see if it works on startup
    # try:
    #     logger.info("--- Gunicorn: Attempting to pre-load default 'catboost' model components on startup ---")
    #     load_model_components('catboost')
    #     logger.info("--- Gunicorn: Default 'catboost' model components pre-loaded successfully ---")
    # except Exception as e_preload:
    #     logger.error(f"--- Gunicorn: FAILED to pre-load 'catboost' model components on startup. Error: {e_preload}", exc_info=True)
    # Pass # No action needed here, Gunicorn will handle serving 'app'
    logger.info("--- Gunicorn: Application instance 'app' is ready to be served. ---")
