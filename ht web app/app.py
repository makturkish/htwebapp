from flask import Flask, request, jsonify, render_template
import pandas as pd
import numpy as np
import joblib
import os
import json
import logging

# Import model classes
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
import lightgbm as lgbm # For lgbm.Booster and LGBMClassifier (though we primarily use Booster for prediction here)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix


logging.basicConfig(level=logging.DEBUG)
app = Flask(__name__)

# Configure base directory for models
# Assumes 'model_output' (generated by training script) is in the same directory as app.py
# Or, provide an absolute path.
MODEL_BASE_DIR = os.path.join(os.path.dirname(__file__), 'model_output')
TARGET_COLUMN = 'hp_category' # Used for metrics calculation if present in uploaded CSV

# --- Model Configuration ---
# Maps frontend model ID to file names and loading specifics
# Note: model_subdir should match the subdirectories created by your training script
MODEL_CONFIG = {
    "catboost": {
        "model_subdir": "catboost_model", # Subdirectory name under MODEL_BASE_DIR
        "model_file": "catboost_model.cbm",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": CatBoostClassifier,
        "load_method": "load_model", # native CatBoost load
        "is_sklearn_joblib": False
    },
    "xgboost": {
        "model_subdir": "xgboost_model",
        "model_file": "xgboost_model.json", # XGBoost can save/load as JSON
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": XGBClassifier,
        "load_method": "load_model", # native XGBoost load
        "is_sklearn_joblib": False
    },
    "random_forest": {
        "model_subdir": "randomforest_model",
        "model_file": "randomforest_model.joblib",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": RandomForestClassifier, # Not actually used for joblib, but good for reference
        "load_method": "joblib",
        "is_sklearn_joblib": True
    },
    "lightgbm": {
        "model_subdir": "lightgbm_model",
        "model_file": "lightgbm_model.txt", # Saved as text by booster_.save_model()
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": None, # We load Booster directly, so specific class not used for instantiation here
        "load_method": "lightgbm_booster",
        "is_sklearn_joblib": False
    },
    "gradient_boosting": {
        "model_subdir": "gradientboosting_model",
        "model_file": "gradientboosting_model.joblib",
        "scaler_file": "scaler.joblib",
        "features_file": "features.json",
        "loader_class": GradientBoostingClassifier, # Not used for joblib
        "load_method": "joblib",
        "is_sklearn_joblib": True
    }
}

# Cache for loaded models to avoid reloading on every request
loaded_models_cache = {}

def load_model_components(model_id):
    """Loads model, scaler, and feature names for a given model_id."""
    if model_id in loaded_models_cache:
        app.logger.info(f"Using cached components for model_id: {model_id}")
        return loaded_models_cache[model_id]

    if model_id not in MODEL_CONFIG:
        raise ValueError(f"Invalid model_id: {model_id}. Not found in MODEL_CONFIG.")

    config = MODEL_CONFIG[model_id]
    model_subdir_path = os.path.join(MODEL_BASE_DIR, config["model_subdir"])

    model_path = os.path.join(model_subdir_path, config["model_file"])
    scaler_path = os.path.join(model_subdir_path, config["scaler_file"])
    features_path = os.path.join(model_subdir_path, config["features_file"])

    if not os.path.exists(model_path): raise FileNotFoundError(f"Model file not found: {model_path}")
    if not os.path.exists(scaler_path): raise FileNotFoundError(f"Scaler file not found: {scaler_path}")
    if not os.path.exists(features_path): raise FileNotFoundError(f"Features file not found: {features_path}")

    model_instance = None
    if config["load_method"] == "load_model": # CatBoost, XGBoost
        ModelClass = config["loader_class"]
        model_instance = ModelClass()
        model_instance.load_model(model_path)
    elif config["load_method"] == "joblib": # RandomForest, GradientBoosting
        model_instance = joblib.load(model_path)
    elif config["load_method"] == "lightgbm_booster":
        # Load the booster directly. This will be our model instance for LightGBM.
        model_instance = lgbm.Booster(model_file=model_path)
    else:
        raise ValueError(f"Unknown load_method: {config['load_method']} for model_id: {model_id}")

    scaler_instance = joblib.load(scaler_path)
    with open(features_path, 'r') as f:
        feature_names_list = json.load(f)

    app.logger.info(f"{model_id.capitalize()} model, scaler, and features loaded successfully.")
    
    components = (model_instance, scaler_instance, feature_names_list)
    loaded_models_cache[model_id] = components # Cache it
    return components

@app.route('/')
def index():
    """Serves the main HTML page."""
    template_path_check = os.path.join(app.root_path, 'templates', 'index.html')
    app.logger.debug(f"Attempting to render template: index.html")
    app.logger.debug(f"Full path to template being considered: {template_path_check}")
    if not os.path.exists(template_path_check):
        app.logger.error(f"Template file DOES NOT EXIST at: {template_path_check}")
        return "Error: Template file 'index.html' not found in 'templates' folder.", 500
    try:
        return render_template('index.html')
    except Exception as e_render:
        app.logger.error(f"Error during render_template('index.html'): {e_render}", exc_info=True)
        return f"Error rendering template: {e_render}", 500

@app.route('/predict', methods=['POST'])
def predict():
    """Handles file upload, prediction, and metrics calculation."""
    if 'file' not in request.files:
        return jsonify({"error": "No file part in the request"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No file selected for uploading"}), 400

    model_id_from_request = request.form.get('model_id', 'catboost') # Default to catboost if not provided
    app.logger.info(f"Received prediction request for model_id: {model_id_from_request}")

    try:
        model, scaler, feature_names = load_model_components(model_id_from_request)
    except Exception as e_load:
        app.logger.error(f"Error loading components for model {model_id_from_request}: {e_load}", exc_info=True)
        return jsonify({"error": f"Could not load model '{model_id_from_request}'. Server not ready or model files missing. Details: {str(e_load)}"}), 500

    if not model or not scaler or not feature_names: # Should be caught by exception above, but as a safeguard
         return jsonify({"error": f"Model components for '{model_id_from_request}' are not properly loaded."}), 500


    if file and file.filename.endswith('.csv'):
        try:
            input_df = pd.read_csv(file.stream)
            original_input_df_for_metrics = input_df.copy()
            
            missing_cols = [col for col in feature_names if col not in input_df.columns]
            if missing_cols:
                return jsonify({"error": f"Missing required columns in CSV: {', '.join(missing_cols)}"}), 400
            
            processed_df = input_df[feature_names].copy()
            for col in feature_names:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')

            if processed_df.isnull().any().any():
                nan_feature_cols = processed_df.columns[processed_df.isnull().any()].tolist()
                app.logger.warning(f"NaN values found in feature columns: {nan_feature_cols}. Applying fillna(0) as default imputation. Ensure this matches training.")
                processed_df.fillna(0, inplace=True) # Basic imputation

            scaled_features = scaler.transform(processed_df)
            
            # Prediction logic adaptable to different model types
            raw_predictions_from_model = model.predict(scaled_features)
            
            predictions = None # Initialize

            if model_id_from_request == "lightgbm" and isinstance(model, lgbm.Booster):
                # lgbm.Booster.predict() for multi-class classification typically returns
                # a 2D array of shape (n_samples, n_classes) with probabilities.
                # We need to find the class with the highest probability.
                if raw_predictions_from_model.ndim == 2 and raw_predictions_from_model.shape[1] > 1:
                    app.logger.info(f"LightGBM booster returned 2D array, shape: {raw_predictions_from_model.shape}. Applying argmax for class labels.")
                    predictions = np.argmax(raw_predictions_from_model, axis=1)
                elif raw_predictions_from_model.ndim == 1:
                    # This might happen if it's binary and returns P(class=1), or if it's already class labels.
                    # For multi-class 'hp_category', this is less likely unless num_class=1 (which is not multi-class).
                    # If your problem IS binary, you might need (raw_predictions_from_model > 0.5).astype(int)
                    app.logger.warning(f"LightGBM booster returned 1D array, shape: {raw_predictions_from_model.shape}. Assuming direct class labels or needs thresholding if binary probabilities.")
                    predictions = raw_predictions_from_model 
                else:
                    app.logger.error(f"Unexpected prediction shape from LightGBM booster: {raw_predictions_from_model.shape}")
                    # Fallback or raise an error; for now, try to use as is if possible.
                    predictions = raw_predictions_from_model # This might need further processing or error if not 1D/2D expected

            elif isinstance(model, CatBoostClassifier):
                if raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] == 1:
                    # CatBoost .predict can return [[0],[1]] for binary classification classes
                    predictions = raw_predictions_from_model.flatten()
                elif raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] > 1:
                    # If CatBoost predict_proba was accidentally used or it's multi-class returning scores per class
                    app.logger.info(f"CatBoost returned multi-column output for predict, shape: {raw_predictions_from_model.shape}. Applying argmax.")
                    predictions = np.argmax(raw_predictions_from_model, axis=1)
                else: # Standard 1D array
                    predictions = raw_predictions_from_model
            
            elif hasattr(model, 'classes_') and raw_predictions_from_model.ndim > 1 and raw_predictions_from_model.shape[1] > 1 : # For scikit-learn models that might return predict_proba like output from .predict
                app.logger.warning(f"Model {model_id_from_request} ({type(model)}) returned multi-column output for .predict(), shape: {raw_predictions_from_model.shape}. Applying argmax.")
                predictions = np.argmax(raw_predictions_from_model, axis=1)
            
            else: # Standard 1D array of class labels from other models (XGBoost, RF, GB usually do this)
                predictions = raw_predictions_from_model

            # Ensure predictions are a flat numpy array of integers
            if predictions is not None:
                predictions = np.array(predictions).flatten().astype(int)
            else:
                # This case should be handled if a model type wasn't processed above
                app.logger.error(f"Predictions array is None after model-specific handling for {model_id_from_request}. This indicates an issue.")
                return jsonify({"error": f"Failed to derive class labels from model {model_id_from_request} output."}), 500

            results_df_for_display = input_df.copy() 
            results_df_for_display['Predicted_Class'] = predictions.tolist()
            response_data = results_df_for_display.to_dict(orient='records')
            
            metrics = None
            if TARGET_COLUMN in original_input_df_for_metrics.columns:
                try:
                    valid_target_indices = original_input_df_for_metrics[TARGET_COLUMN].dropna().index
                    y_true, y_pred = pd.Series([], dtype=int), pd.Series([], dtype=int) # Initialize with dtype

                    if len(predictions) == len(original_input_df_for_metrics):
                        y_true_all = original_input_df_for_metrics.loc[valid_target_indices, TARGET_COLUMN].astype(int)
                        # Ensure predictions Series has same index as original_input_df for correct alignment
                        predictions_series = pd.Series(predictions, index=original_input_df_for_metrics.index)
                        y_pred_all = predictions_series.loc[valid_target_indices].astype(int)
                        
                        common_indices = y_true_all.index.intersection(y_pred_all.index)
                        y_true = y_true_all.loc[common_indices]
                        y_pred = y_pred_all.loc[common_indices]
                    else:
                        app.logger.error("Mismatch in length between predictions and input data. Metrics might be affected.")

                    if not y_true.empty and not y_pred.empty:
                        accuracy = accuracy_score(y_true, y_pred)
                        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
                        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0) # This is Sensitivity (Macro)
                        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
                        
                        # Ensure cm_labels are derived from the actual data used for CM
                        cm_labels_unique = sorted(np.unique(np.concatenate((y_true.unique(), y_pred.unique()))).tolist())
                        if not cm_labels_unique: # Handle case where y_true/y_pred might be empty after all filtering
                             cm_labels_unique = [0] # Default to at least one label if completely empty, or handle as error
                        
                        cm = confusion_matrix(y_true, y_pred, labels=cm_labels_unique)
                        
                        per_class_specificity = []
                        if cm.size > 0 and len(cm_labels_unique) == cm.shape[0] and len(cm_labels_unique) == cm.shape[1]: # Check CM shape against labels
                            for i, label_val_cm in enumerate(cm_labels_unique): # Iterate based on cm_labels_unique
                                tp = cm[i, i]
                                fp = cm[:, i].sum() - tp
                                fn = cm[i, :].sum() - tp
                                tn = cm.sum() - (tp + fp + fn)
                                specificity_i = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                                per_class_specificity.append(specificity_i)
                        else: # CM shape doesn't match labels, or CM is empty.
                            app.logger.warning(f"CM shape ({cm.shape}) or cm_labels_unique ({len(cm_labels_unique)}) mismatch or CM empty. Specificity might be 0 or inaccurate.")
                            # Fill with NaNs or 0s for each expected label if CM is problematic
                            per_class_specificity = [np.nan] * len(cm_labels_unique)


                        specificity_macro = np.nanmean(per_class_specificity) if per_class_specificity else 0.0
                        
                        metrics = {
                            "accuracy": f"{accuracy:.4f}",
                            "precision_macro": f"{precision_macro:.4f}",
                            "sensitivity_macro": f"{recall_macro:.4f}", 
                            "specificity_macro": f"{specificity_macro:.4f}",
                            "f1_macro": f"{f1_macro:.4f}",
                            "confusion_matrix": cm.tolist(),
                            "cm_labels": cm_labels_unique,
                            "message": f"Metrics calculated using '{TARGET_COLUMN}' from uploaded file for model '{model_id_from_request}'."
                        }
                    else:
                        metrics = {"message": f"Target column '{TARGET_COLUMN}' found, but no aligned (y_true, y_pred) data for metric calculation after processing."}
                except Exception as e_metrics:
                    app.logger.error(f"Error calculating metrics for {model_id_from_request}: {e_metrics}", exc_info=True)
                    metrics = {"error": f"Could not calculate metrics: {str(e_metrics)}"}
            else:
                metrics = {"message": f"Target column '{TARGET_COLUMN}' not found in uploaded file. Metrics not calculated."}

            return jsonify({
                "predictions": response_data, 
                "headers": results_df_for_display.columns.tolist(),
                "metrics": metrics,
                "model_used": model_id_from_request
            })

        except pd.errors.EmptyDataError: return jsonify({"error": "Uploaded CSV file is empty."}), 400
        except Exception as e_process:
            app.logger.error(f"Error during prediction processing for {model_id_from_request}: {e_process}", exc_info=True)
            return jsonify({"error": f"An error occurred: {str(e_process)}"}), 500
    else:
        return jsonify({"error": "Invalid file type. Please upload a CSV file."}), 400

if __name__ == '__main__':
    app.logger.info("--- Flask App Starting ---")
    app.logger.info(f"Models will be loaded from: {MODEL_BASE_DIR}")
    app.logger.info(f"To run, ensure the 'model_output' directory (from training) is present here or MODEL_BASE_DIR is correctly set.")
    app.logger.info(f"Expected subdirectories in '{MODEL_BASE_DIR}': {', '.join([MODEL_CONFIG[key]['model_subdir'] for key in MODEL_CONFIG])}")
    app.run(debug=True, host='0.0.0.0') # Listen on all available IPs for easier access if running in a container/VM
